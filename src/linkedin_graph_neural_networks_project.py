# -*- coding: utf-8 -*-
"""Linkedin Graph Neural Networks Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a1V229rw7qFSgOZEej05VIZKsePwit3a

# Introduction

This a personal project created by [Alexander Quesada Quesada](https://www.linkedin.com/in/alexander-quesada-quesada-b91348259/), the goal is to simulate Linkedin job postings as nodes in a graph and create edges between nodes that satisfy specific requierments, those connections represent similarity between two jobs, and they will also have weights from 0 to 1 representing the strength of that similarity.

**This project includes two different types of Deep Learning Models:**

* GNN (Graph Neural Network) model that predicts new connections between nodes after a cautious and meticulous training process where it learns the patterns of the pre-established edges to come up with new ones.

* XGBoost model that applies regression learning to predict  weights (strength) for all the previously predicted
new connections between nodes.

**The workflow of this project consists on:**  

1. Loading and Merging the CSV different files into pandas datasets.

2. Cleaning, encoding and polishing further details of the merged pandas dataset.

3. Preparing the data as matrices that PyTorch Geometric library can understand and convert into [PyTorch Geometric Graph Data](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html).

4. Plotting some examples of the PyTorch Geometric Graph Data

5. Preparing both Training and Testing Data.

6. [Model Creation (by Orbifold)](https://github.com/Orbifold/pyg-link-prediction/blob/main/run.py#L22), batching, training and evaluating GNN model's performance.

7. Creating and Training the XGBoost model.

8. Create the final function that predicts all possible new connections and their corresponding weights for all nodes within some input graph, by using the previous mentioned models.

9. Unit Testing the combined Deep Learning Pipeline's predictions and perfomance.

10. Use [GNNLens2](https://github.com/dmlc/GNNLens2) for graph visualization, with the Deep Learning Pipeline's results included.

11. Public Repository Creating and Web Deployment of this App.

12. Models Pipeline Overview by Visualization [(README)](https://github.com/papitaAlgodonCplusplus/Linkedin-Graph-Neural-Networks-Project/blob/main/README.md)

# Dataset and Files

---

[This dataset](https://www.kaggle.com/datasets/arshkon/linkedin-job-postings) comprises over 33,000 job postings gathered from LinkedIn over two separate days, offering a comprehensive snapshot of job opportunities. Each posting includes 27 attributes such as title, job description, salary, location, application URL, and work type (remote, contract, etc). Additional files contain information on benefits, skills, and industries associated with each posting. The dataset also links jobs to companies, with a separate CSV file providing details on each company, including description, headquarters location, number of employees, and follower count.

---

### job_postings.csv

- `job_id`: The job ID as defined by [LinkedIn](https://www.linkedin.com/jobs/view/job_id).
- `company_id`: Identifier for the company associated with the job posting (maps to companies.csv).
- `title`: Job title.
- `description`: Job description.
- `max_salary`: Maximum salary.
- `med_salary`: Median salary.
- `min_salary`: Minimum salary.
- `pay_period`: Pay period for salary (Hourly, Monthly, Yearly).
- `formatted_work_type`: Type of work (Fulltime, Parttime, Contract).
- `location`: Job location.
- `applies`: Number of applications that have been submitted.
- `original_listed_time`: Original time the job was listed.
- `remote_allowed`: Whether the job permits remote work.
- `views`: Number of times the job posting has been viewed.
- `job_posting_url`: URL to the job posting on a platform.
- `application_url`: URL where applications can be submitted.
- `application_type`: Type of application process (offsite, complex/simple onsite).
- `expiry`: Expiration date or time for the job listing.
- `closed_time`: Time to close job listing.
- `formatted_experience_level`: Job experience level (entry, associate, executive, etc).
- `skills_desc`: Description detailing required skills for the job.
- `listed_time`: Time when the job was listed.
- `posting_domain`: Domain of the website with the application.
- `sponsored`: Whether the job listing is sponsored or promoted.
- `work_type`: Type of work associated with the job.
- `currency`: Currency in which the salary is provided.
- `compensation_type`: Type of compensation for the job.
â€Ž

### job_details/benefits.csv

- `job_id`: The job ID.
- `type`: Type of benefit provided (401K, Medical Insurance, etc).
- `inferred`: Whether the benefit was explicitly tagged or inferred through text by LinkedIn.

### company_details/companies.csv

- `company_id`: The company ID as defined by LinkedIn.
- `name`: Company name.
- `description`: Company description.
- `company_size`: Company grouping based on the number of employees (0 Smallest - 7 Largest).
- `country`: Country of the company headquarters.
- `state`: State of the company headquarters.
- `city`: City of the company headquarters.
- `zip_code`: ZIP code of the company's headquarters.
- `address`: Address of the company's headquarters.
- `url`: Link to the company's LinkedIn page.

### company_details/employee_counts.csv

- `company_id`: The company ID.
- `employee_count`: Number of employees at the company.
- `follower_count`: Number of company followers on LinkedIn.
- `time_recorded`: Unix time of data collection.

---

*CC BY-SA 4.0*

*By ARSH KON*

---

# Load -> Transform -> Merge

## Job Postings CSV
"""

import pandas as pd
import csv
df = pd.read_csv("https://git.ucr.ac.cr/ALEXANDER.QUESADAQUESADA/misc/-/raw/main/job_postings.csv?ref_type=heads", on_bad_lines='skip')

df

# Keep columns that have at least 50% of non NaN data
df.dropna(axis=1, thresh=int(15886*0.5), inplace=True)
df

# Fill nan applies as 0
df['applies'].fillna(value=0, inplace=True)
df['applies'] = df['applies'].astype(int)

# Drop useless columns
df.drop(columns='original_listed_time', inplace=True)
df.drop(columns='job_posting_url', inplace=True)
df.drop(columns='application_url', inplace=True)
df.drop(columns='expiry', inplace=True)
df.drop(columns='listed_time', inplace=True)
df.drop(columns='work_type', inplace=True)
df

"""## Companies CSV"""

df2 = pd.read_csv("https://raw.githubusercontent.com/papitaAlgodonCplusplus/Misc/main/companies.csv", on_bad_lines='skip')
df2

df = df.merge(df2, on='company_id', how='left')
df.rename(columns = {'description_x':'job_desc'}, inplace = True)
df.rename(columns = {'description_y':'company_desc'}, inplace = True)
df.rename(columns = {'name':'company_name'}, inplace = True)
df.rename(columns = {'state':'company_state'}, inplace = True)
df.rename(columns = {'country':'company_country'}, inplace = True)
df.rename(columns = {'city':'company_city'}, inplace = True)
df.rename(columns = {'zip_code':'company_zip_code'}, inplace = True)
df.rename(columns = {'address':'company_address'}, inplace = True)
df.drop(columns='url', inplace=True)
del df2
df

"""## Benefits CSV"""

df3 = pd.read_csv("https://raw.githubusercontent.com/papitaAlgodonCplusplus/Misc/main/benefits.csv", on_bad_lines='skip')
df3

df = df.merge(df3, on='job_id', how='left')
df.rename(columns = {'type':'job_benefit'}, inplace = True)
df.drop(columns='inferred', inplace=True)
del df3
df

"""## Skills CSV"""

df4 = pd.read_csv("https://raw.githubusercontent.com/papitaAlgodonCplusplus/Misc/main/job_skills.csv", on_bad_lines='skip')
df4

df = df.merge(df4, on='job_id', how='left')
df.rename(columns = {'skill_abr':'job_skill_type'}, inplace = True)
del df4
df

"""## Companies roles"""

df5 = pd.read_csv("https://raw.githubusercontent.com/papitaAlgodonCplusplus/Misc/main/company_specialities.csv", on_bad_lines='skip')
df5 = df5.groupby('company_id')['speciality'].agg(', '.join).reset_index()
df5

df = df.merge(df5, on='company_id', how='left')
df.rename(columns = {'speciality':"company's roles"}, inplace = True)
del df5
df

"""## Companies role (official)"""

df6 = pd.read_csv("https://raw.githubusercontent.com/papitaAlgodonCplusplus/Misc/main/company_industries.csv", on_bad_lines='skip')
df6

df = df.merge(df6, on='company_id', how='left')
df.rename(columns = {'industry':"company's industry"}, inplace = True)
del df6
df

# Clean data
df = df.dropna(subset=['company_id'])
# df

"""## Fill NA

**A more sophisticated way to fill NaNs is to use NLP to extract info from job description, but for time reasons, I'll do as:**
"""

# Numerical
df['views'].fillna(value=0, inplace=True)
df['company_zip_code'].fillna(value=0, inplace=True)
df['views'] = df['views'].astype(int)
df['company_id'] = df['company_id'].astype(int)

# Drop columns (useless for this case)
df.drop(columns='posting_domain', inplace=True)

# Text Format

# Mode
df['formatted_experience_level'].fillna(df['formatted_experience_level'].mode()[0], inplace=True)
df['company_state'].fillna(df['company_state'].mode()[0], inplace=True)
df['company_country'].fillna(df['company_country'].mode()[0], inplace=True)
df['company_city'].fillna(df['company_city'].mode()[0], inplace=True)
df['company_size'].fillna(df['company_size'].mode()[0], inplace=True)
df['company_address'].fillna(df['company_address'].mode()[0], inplace=True)
df['job_skill_type'].fillna(df['job_skill_type'].mode()[0], inplace=True)
df["company's industry"].fillna(df["company's industry"].mode()[0], inplace=True)

# Specific Value
df['job_desc'].fillna('no_job_desc', inplace=True)
df['company_name'].fillna('generic_company', inplace=True)
df['company_desc'].fillna('no_company_desc', inplace=True)
df['job_benefit'].fillna('None', inplace=True)
df["company's roles"].fillna("None", inplace=True)
df

"""## Row - Column Playground"""

row_index = 22
column_name = 'job_desc'
value = df.iloc[row_index]#[column_name]
print(value)

"""## NA values verification"""

nan_counts = {}

for column in df.columns:
    nan_count = df[column].isna().sum()
    nan_counts[column] = nan_count

for column, count in nan_counts.items():
    print(f"Number of NaN values in '{column}': {count}")

"""## Remove Duplicates"""

df.drop_duplicates(subset='job_id', keep='first', inplace=True, ignore_index=False)
df.reset_index(inplace=True)
df.drop(columns='index', inplace=True)
df

"""# GNN Data Preparation

For this project, I'll use a **Homogeneous Graph**, given the choice of:

- `Nodes` - Jobs (by ID)
- `Edges` - If they are within the same category (job_skill_type, binary title correlation, company's industry, company's location)
- `Edges's Weight` - The amount of correlations between two jobs
- `Node Features` - All company's and job's features except for job's ID and descriptions (As description is not a feature worth to hot-encode, nor is easy to extract specific information of highly variable text formats)
- `Labels` - Predict a node's edges and it's weights


"""

# Sort to define the order of nodes
sorted_df = df.sort_values(by="job_id")
# Select node features
node_features = sorted_df.loc[:, sorted_df.columns != 'job_id']
del sorted_df
node_features.head(45)

"""## Job Title Encoding

I tried to apply NER with transformers and spacy here to 'summarize' the job title into one per-category word that could bag all related jobs, however, given the 'unordered' format of the job titles this is a complex task that requieres more dedication and probably hybrid solutions, so, instead I'll one-hot encode the most common words among the titles.
"""

# Use str.get_dummies() to encode the 'title' column
title_dummies = df['title'].str.get_dummies(sep=' ')

# Print the resulting DataFrame
print(title_dummies)

"""### Keep columns with at least 0.25% of frecuency

"""

threshold = len(title_dummies) * 0.0025
pd.options.mode.chained_assignment = None  # Disables the warning

# Filter columns based on the threshold
title_dummies_filtered = title_dummies.loc[:, (title_dummies.sum() >= threshold)]
# Drop special chars columns
title_dummies_filtered.drop(columns=[col for col in title_dummies_filtered.columns if len(col) == 1], inplace=True)
# Drop stop words
title_dummies_filtered.drop(columns='and', inplace=True)
title_dummies_filtered.drop(columns='of', inplace=True)
title_dummies_filtered.drop(columns='for', inplace=True)
title_dummies_filtered.drop(columns='2024', inplace=True)
title_dummies_filtered.drop(columns='per', inplace=True)
title_dummies_filtered.drop(columns='in', inplace=True)
title_dummies_filtered.drop(columns='to', inplace=True)
del title_dummies
# Print the resulting DataFrame
print(title_dummies_filtered)

"""### Merge"""

node_features = pd.merge(node_features, title_dummies_filtered, left_index=True, right_index=True, how='left')
node_features.drop(columns='title', inplace=True)
del title_dummies_filtered

# Since it's impossible to summarize a whole job description into a reasonable amount of columns
# Nor encode them in a numerical way using complex NLP (Transfomer based) for time reasons
# We won't use this column for features
node_features.drop(columns='job_desc', inplace=True)
# Same applies for company's description
node_features.drop(columns='company_desc', inplace=True)
# Also, since we have company's id, company's name is obsolete
node_features.drop(columns='company_name', inplace=True)
node_features

"""## One Column Encoding"""

# Work Type
unique_work_types = node_features['formatted_work_type'].unique()
mapping = {work_type: i for i, work_type in enumerate(unique_work_types)}

node_features['formatted_work_type'] = node_features['formatted_work_type'].map(mapping)

# Location
node_features[['city', 'state']] = node_features['location'].str.split(', ', n=1, expand=True)
node_features.drop(columns='location', inplace=True)

uniques = node_features['city'].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features['city'] = node_features['city'].map(mapping)

# Company Country
uniques = node_features['company_country'].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features['company_country'] = node_features['company_country'].map(mapping)

# Experience Level
uniques = node_features['formatted_experience_level'].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features['formatted_experience_level'] = node_features['formatted_experience_level'].map(mapping)

# Company State
uniques = node_features['company_state'].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features['company_state'] = node_features['company_state'].map(mapping)

uniques = node_features['state'].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features['state'] = node_features['state'].map(mapping)

# Application Type
uniques = node_features['application_type'].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features['application_type'] = node_features['application_type'].map(mapping)

# Job Required Skill
uniques = node_features['job_skill_type'].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features['job_skill_type'] = node_features['job_skill_type'].map(mapping)

# Job Benefit
uniques = node_features['job_benefit'].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features['job_benefit'] = node_features['job_benefit'].map(mapping)

# Company Size
node_features['company_size'] = node_features['company_size'].astype(int)

# Company's city
uniques = node_features['company_city'].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features['company_city'] = node_features['company_city'].map(mapping)

# Company's Industry
uniques = node_features["company's industry"].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features["company's industry"] = node_features["company's industry"].map(mapping)

# Company's Roles
def remove_duplicates(row):
    roles = row.split(', ')
    unique_roles = list(set(roles))
    return ', '.join(unique_roles)

node_features["company's roles"] = node_features["company's roles"].apply(remove_duplicates)

uniques = node_features["company's roles"].unique()
mapping = {unique_value: i for i, unique_value in enumerate(uniques)}
node_features["company's roles"] = node_features["company's roles"].map(mapping)

# Drop redundant info
node_features.drop(columns='company_address', inplace=True)

del uniques
del mapping
node_features

# Convert to numpy
x = node_features.to_numpy()
x.shape

"""## Edges Creation

### Edges Weights

We create 4 matrices based on 4 different columns of the nodes, then we apply a relevance-conscious merge to get a final matrix that tell us the strength of the edge between node a and b.

#### job_skill_type
"""

import itertools
import numpy as np

adjacency_matrix_job_types = np.zeros((15520, 15520), dtype=np.int32)

job_types = node_features['job_skill_type'].unique()

for i in job_types:
  # Subset of rows within the same job skill type
  df_by_job_type_temp = node_features[node_features['job_skill_type'] == i]

  # Get index of all rows of the subset as array
  row_indexes = df_by_job_type_temp.index.to_numpy()

  # row_indexes[:, np.newaxis] = [1, 3, 56, 67] -> [[1], [3], [56], [67]]
  adjacency_matrix_job_types[row_indexes[:, np.newaxis], row_indexes] = 1

# Avoid self edges (identity matrix)
adjacency_matrix_job_types[np.arange(15520), np.arange(15520)] = 0

# Display without hiding
# np.set_printoptions(threshold=np.inf)

"""#### binary title correlation"""

title_sub_df = node_features.loc[:, '(Hybrid)':'{Owner/Operator}']
adjacency_matrix_title = np.zeros((len(title_sub_df.columns), 15520, 15520), dtype=float)

for index, column_name in enumerate(title_sub_df.columns):
  df_by_title_temp = node_features[node_features[column_name] == 1]
  row_indexes = df_by_title_temp.index.to_numpy()
  adjacency_matrix_title[index, row_indexes[:, np.newaxis], row_indexes] = 1

# Display without hiding
# np.set_printoptions(threshold=np.inf)

# row_indices, col_indices = np.where(adjacency_matrix_title[0] == 1)

import gc
gc.collect()

n = len(title_sub_df.columns)
for i in range(len(title_sub_df.columns)):
  adjacency_matrix_title[n-1] = \
  adjacency_matrix_title[n-1] + adjacency_matrix_title[i]

gc.collect()
adjacency_matrix_title = adjacency_matrix_title[n-1]
gc.collect()
adjacency_matrix_title[np.arange(15520), np.arange(15520)] = 0

title_sub_df['Sum'] = title_sub_df.sum(axis=1)
sum_array = title_sub_df['Sum'].to_numpy()

gc.collect()
adjacency_matrix_title /= np.maximum.outer(sum_array, sum_array)
adjacency_matrix_title = np.round(adjacency_matrix_title, decimals=2)

adjacency_matrix_title = np.nan_to_num(adjacency_matrix_title, nan=0)
print(np.max(adjacency_matrix_title))

"""#### company's industry"""

adjacency_matrix_company_industry = np.zeros((15520, 15520), dtype=np.int32)

industry_types = node_features["company's industry"].unique()

for i in industry_types:
  df_by_industry_temp = node_features[node_features["company's industry"] == i]

  # Get index of all rows of the subset as array
  row_indexes = df_by_industry_temp.index.to_numpy()

  # row_indexes[:, np.newaxis] = [1, 3, 56, 67] -> [[1], [3], [56], [67]]
  adjacency_matrix_company_industry[row_indexes[:, np.newaxis], row_indexes] = 1

# Avoid self edges (identity matrix)
adjacency_matrix_company_industry[np.arange(15520), np.arange(15520)] = 0

#for i in range(20):
#  print()
#  for j in range(0,20):
#    print(adjacency_matrix_company_industry[i,j], end=' ')

"""#### company's roles"""

adjacency_matrix_company_roles = np.zeros((15520, 15520), dtype=np.int32)

roles_types = node_features["company's roles"].unique()

for i in roles_types:
  df_by_roles_temp = node_features[node_features["company's roles"] == i]

  # Get index of all rows of the subset as array
  row_indexes = df_by_roles_temp.index.to_numpy()

  # row_indexes[:, np.newaxis] = [1, 3, 56, 67] -> [[1], [3], [56], [67]]
  adjacency_matrix_company_roles[row_indexes[:, np.newaxis], row_indexes] = 1

# Avoid self edges (identity matrix)
adjacency_matrix_company_roles[np.arange(15520), np.arange(15520)] = 0

"""#### Final Weights Matrix"""

final_weights_matrix = np.zeros((15520, 15520), dtype=float)
final_weights_matrix = (adjacency_matrix_job_types  * 0.25) + ((adjacency_matrix_title/2) * 0.65) \
+ (adjacency_matrix_company_industry * 0.05) + (adjacency_matrix_company_roles * 0.05)

del adjacency_matrix_job_types
del adjacency_matrix_title
del adjacency_matrix_company_industry
del adjacency_matrix_company_roles
gc.collect()

final_weights_matrix = np.round(final_weights_matrix, decimals=8)
final_weights_matrix = np.nan_to_num(final_weights_matrix, nan=0)
final_weights_matrix[np.arange(15520), np.arange(15520)] = 0
for i in range(20,50):
  print()
  for j in range(55,85):
    print(final_weights_matrix[i,j], end=' ')

"""### Edges

Just a matrix that has 1 if the weights matrix cell is not 0, 0 otherwise.
"""

edges_matrix = (final_weights_matrix != 0).astype(int)
for i in range(20,50):
  print()
  for j in range(55,85):
    print(edges_matrix[i,j], end=' ')

repeated_indices = []
edges_indices = []

for i in range(15520):
    row = edges_matrix[i]
    ones_indices = np.where(row == 1)[0]
    repeated_indices.extend([i] * len(ones_indices))
    edges_indices.extend(ones_indices)

result_array = np.array(repeated_indices)
edges_array = np.array(edges_indices)

del repeated_indices
del edges_indices
gc.collect()

all_edges = np.array((result_array, edges_array), dtype = np.int32)

print(all_edges.shape)

"""## Label Extraction

0: [0, 0, 0, 1]

1: [0, 0, 1, 0]

Labels =

0-0, 0-1, 0-2, 0-3, 1-0, 1-1...

[0, 0, 0, 5, 0, 0, 6, 0]
"""

labels = final_weights_matrix[final_weights_matrix != 0.0].flatten()
print(labels.shape)

gc.collect()

"""# GNN"""

!pip install torch-geometric

# Initialize a dictionary to map non-numeric values to unique numerical values
non_numeric_values = {}
numeric_value = 0

for i in range(x.shape[0]):
    for j in range(x.shape[1]):
        value = x[i, j]
        if not np.issubdtype(type(value), np.number):
            if value not in non_numeric_values:
                non_numeric_values[value] = numeric_value
                numeric_value += 1
            x[i, j] = non_numeric_values[value]

from torch_geometric.data import Data
from torch_geometric.loader import DataLoader

data = Data(x=x, edge_index=all_edges, y=labels)
print(data) # 48806450 means 20.28% out of all (15520*15520) possible edges

import pickle
with open('original_linkedin_graph_data', 'wb') as f:
    pickle.dump(data, f)

# Accessing example values
print("Node features (x):", data.x[0][:5])  # Print the first 5 features of the first node
print("Edge connections (edge_index):", data.edge_index[:, :5])  # Print the first 5 edges
print("Edge labels (y):", data.y[:5])  # Print the labels of the first 5 edges
print("\n\n\n")
print("Node features (x2):", data.x[1][:5])  # Print the first 5 features of the second node
print("Edge connections (edge_index):", data.edge_index[:, 6855:6860])  # Print some edges of the second node
print("Edge labels (y2):", data.y[6855:6860])  # Print the labels of the previous edges

"""## Plot Sample Graph"""

import torch
import torch_geometric.transforms as T
from torch_geometric.utils import to_networkx
import torch_geometric.utils as utils
import networkx as nx
import matplotlib.pyplot as plt
import matplotlib as mpl
import random
import math

del all_edges
del df_by_industry_temp
del df_by_job_type_temp
del df_by_roles_temp
del df_by_title_temp
del edges_array
del labels
del node_features
del row
del sum_array
del title_sub_df
del result_array
gc.collect()

# Create a list to store all possible pairs where adjacency is 1
possible_pairs = [(i, j) for i, row in enumerate(final_weights_matrix) \
                         for j, val in enumerate(row[i+1:], start=i+1) if val != 0]

random_sample_pairs = random.sample(possible_pairs, 30)

unique_values = {value for tuple_ in random_sample_pairs for value in tuple_}
unique_edges_list = list(unique_values)
random_sample_pairs = [(i, j) for i in unique_edges_list for j in unique_edges_list if i != j]

# Shuffle the result list randomly
random.shuffle(random_sample_pairs)

# Slice the list to keep the fisrt 3/4 tuples
random_sample_pairs = random_sample_pairs[:(len(unique_values) // 2)]

i_indices, j_indices = zip(*random_sample_pairs)
alpha_values = final_weights_matrix[i_indices, j_indices]

unique_values = {value for tuple_ in random_sample_pairs for value in tuple_}
unique_edges_list = list(unique_values)

G = nx.Graph()  # You can use `DiGraph` for directed graphs

# Add edges from the numpy array
G.add_edges_from(random_sample_pairs)

print("Number of nodes:", G.number_of_nodes())
print("Number of edges:", G.number_of_edges())
print(alpha_values)

pos = nx.spring_layout(G, k=0.35, seed=6)

# Draw labels with personalized pos
# https://stackoverflow.com/questions/14547388/networkx-in-python-draw-node-attributes-as-labels-outside-the-node

node_sizes = 100
M = G.number_of_edges()
edge_colors = range(2, M + 2)
cmap = plt.cm.plasma

nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color="indigo")
edges = nx.draw_networkx_edges(
    G,
    pos,
    node_size=node_sizes,
    edge_color=edge_colors,
    edge_cmap=cmap,
    width=4,
)

def truncate_to_two_words(input_string):
    words = input_string.split()
    truncated_string = ' '.join(words[:2])
    return truncated_string

labels = nx.draw_networkx_labels(G, pos, labels={n: truncate_to_two_words(job_name) for n, job_name in
                                 zip(G, df.iloc[unique_edges_list]['title'])}
                              , font_size=8, font_color='k', font_family='fantasy',
                              font_weight='normal', alpha=None, bbox=None, horizontalalignment='left',
                              verticalalignment='baseline', ax=None, clip_on=False)
# set alpha value for each edge
i = 0
for line in edges.get_paths():
    edges.set_alpha(alpha_values[i])
    i = i+1

ax = plt.gca()
ax.set_axis_off()
plt.show()

"""## Val / Eval / Train Split

### Training Data
"""

# Select the first 13968 nodes (90%)
selected_nodes = data.x[:13968]

# Find the index of the last occurrence of 13968 in the first row

# data.edge_index[0] == 13968: This creates a boolean mask by comparing each element in the first row of data.edge_index
# with the value 13968. The result is a boolean tensor of the same shape as data.edge_index[0], where True indicates
# that the corresponding element is equal to 13968.

# The nonzero function returns a tensor containing the indices where the condition is True.
# The as_tuple=False argument ensures that the result is a tensor rather than a tuple.

# [-1, 0]: This accesses the last row and the first column of the tensor obtained from the previous step.
# This is done to extract the index of the last occurrence of True in the boolean tensor,
# which corresponds to the last occurrence of 13968 in the first row of data.edge_index.

# .item() extracts the scalar tensor as an Integer
last_index = (data.edge_index[0] == 13968).nonzero()
numpy_array = last_index[0]
last_index = numpy_array[-1]
print(last_index)

# Select columns from the first 0 to the last occurrence of 13968 in the first row
# +1 is because the 0...n conviction, where colums in this context are from 1...n+1
selected_edges = data.edge_index[:, :last_index+1]

# Select the first 13968 labels
selected_labels = data.y[:last_index+1]

# Create a new Data object with the selected data
training_data = Data(x=selected_nodes, edge_index=selected_edges, y=selected_labels)

# Print the selected data
print(training_data)

# Accessing example values
print("Node features (x):", training_data.x[0][-5:])
print("Edge connections (edge_index):", training_data.edge_index[:, -5:])
print("Edge labels (y):", training_data.y[-5:])
print("\n\n\n")

"""### Testing Data"""

last_index = (data.edge_index[0] == 15519).nonzero()
numpy_array = last_index[0]
last_index = numpy_array[-1]
print(last_index)

first_index = (data.edge_index[0] == 13969).nonzero()
numpy_array = first_index[0]
first_index = numpy_array[0]
print(first_index)

selected_edges = data.edge_index[:, first_index:last_index+1]

selected_labels = data.y[first_index:last_index+1]

# Create a new Data object with the selected data
testing_data = Data(x=data.x, edge_index=selected_edges, y=selected_labels)

# Print the selected data
print(testing_data)

# Accessing example values
print("Node features (x):", testing_data.x[0][:5])
print("Edge connections (edge_index):", testing_data.edge_index[:, :5])
print("Edge labels (y):", testing_data.y[:5])
print("\n\n\n")
print("Node features (x):", testing_data.x[0][-5:])
print("Edge connections (edge_index):", testing_data.edge_index[:, -5:])
print("Edge labels (y):", testing_data.y[-5:])

"""## Model Creation"""

from torch_geometric.nn import GCNConv
class Net(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def encode(self, x, edge_index):
        # chaining two convolutions with a standard relu activation
        x = self.conv1(x, edge_index).relu()
        return self.conv2(x, edge_index)

    def decode(self, z, edge_label_index):
        # cosine similarity
        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim = -1)

"""## Memory Cleaning

"""

del edges
del df
del edges_matrix
del ones_indices
del possible_pairs
del random_sample_pairs
del last_index
del roles_types
del numpy_array
del first_index
del unique_edges_list
del unique_values
del selected_edges
del selected_labels
gc.collect()

del G
del ax
del cmap
del edge_colors
del i
del i_indices
del index
del industry_types
del job_types
del labels
del line
del n
del nan_count
del nan_counts
del node_sizes
del pos
del row_index
del row_indexes
del value
gc.collect()

import time
# Let the RAM update
time.sleep(30)

"""## Training Data Manual Batch Split"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install tqdm

import torch

# Convert NumPy arrays to PyTorch tensors with compatible data types
x_tensor = torch.tensor(training_data.x.astype(np.float32), dtype=torch.float32)
edge_index_tensor = torch.tensor(training_data.edge_index, dtype=torch.long)

# Assign the tensors to the data object
training_data.x = x_tensor
training_data.edge_index = edge_index_tensor

# Convert NumPy arrays to PyTorch tensors with compatible data types
x_tensor = torch.tensor(testing_data.x.astype(np.float32), dtype=torch.float32)
edge_index_tensor = torch.tensor(testing_data.edge_index, dtype=torch.long)

# Assign the tensors to the data object
testing_data.x = x_tensor
testing_data.edge_index = edge_index_tensor

import torch
from torch_geometric.data import Data
from tqdm import tqdm

batch_size = 32
num_samples = 13968
# +batch_size in case of needing x.yyy batches, so it would be x+batch_size batches
num_batches = (num_samples + batch_size) // batch_size
subDatas = []
edge_index = training_data.edge_index

for i in tqdm(range(num_batches), desc='Processing', unit='batch'):
    start_idx = i * batch_size
    end_idx = min((i + 1) * batch_size, num_samples)

    # Create a boolean mask for rows where values are within the range [start_idx, end_idx]
    row_mask = (edge_index[0, :] >= start_idx) & (edge_index[0, :] < end_idx) \
            & (edge_index[1, :] >= start_idx) & (edge_index[1, :] < end_idx)

    # Apply the boolean mask to filter rows
    batch_edge_index = edge_index[:, row_mask]
    batch_y = training_data.y[row_mask]

    # Create a new Data object for the batch
    batch_data = Data(x=training_data.x, edge_index=batch_edge_index, y=batch_y)

    # Append the batch Data object to the list
    subDatas.append(batch_data)

print(subDatas)

print(subDatas[0].x.shape)

print(subDatas[0].edge_index.shape)

print(subDatas[1].y)

"""## Training"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv


# Instantiate the model
num_features = 255
hidden_dim = 64
num_classes = 1
model = Net(num_features, hidden_dim, num_classes)

"""Graph Neural Networks (GNNs) are often trained using a process known as "negative sampling" to improve their ability to distinguish between positive and negative examples, especially in the context of **link prediction** or node classification tasks. Negative sampling is a technique used to address the **class imbalance inherent** in graph-structured data.

<font color='red'>Negative sampling</font> is relevant primarily for tasks involving binary classification, such as predicting whether an edge should exist between two nodes in a graph. The basic idea is to create negative examples (pairs of nodes that do not have an edge) to balance the <font color='green'>positive examples</font> (pairs of nodes that have an edge). This helps prevent the model from becoming biased towards predicting positive examples, especially when the number of negative examples significantly outweighs the number of positive examples in the dataset.

In this scenario, I'll be using <font color='blue'>Random Negative Sampling</font> as it is a straight-forward class imbalance addresser, especial for dealing with tasks involving binary classification.
"""

import torch
import torch.nn as nn
from torch_geometric.utils import negative_sampling
import plotly.graph_objects as go

criterion = nn.MSELoss()  # Mean Squared Error Loss for regression
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
epochs = 30
min_loss = float('inf')

# Store loss values for plotting
losses = []

for epoch in range(epochs):
    model.train()

    # Forward pass
    for i in range(len(subDatas)):
        optimizer.zero_grad()
        z = model.encode(subDatas[i].x, subDatas[i].edge_index)
        neg_edge_index = negative_sampling(edge_index=subDatas[i].edge_index, num_nodes=subDatas[i].num_nodes,
                                           num_neg_samples=None, method='sparse')
        edge_label_index = torch.cat([subDatas[i].edge_index, neg_edge_index], dim=-1, )
        edge_label = torch.cat([torch.ones(subDatas[i].edge_index.size(1)), torch.zeros(neg_edge_index.size(1))],
                               dim=0)
        out = model.decode(z, edge_label_index).view(-1)
        loss = criterion(out, edge_label)
        loss.backward()
        optimizer.step()
        if loss.item() < min_loss:
            min_loss = loss.item()
            torch.save(model.state_dict(), 'best_model.pth')

    # Append current loss to the list
    losses.append(loss.item())
    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')

# Plotting the loss curve using Plotly
fig = go.Figure()
fig.add_trace(go.Scatter(x=list(range(1, epochs + 1)), y=losses, mode='lines+markers'))
fig.update_layout(title='Loss Over Epochs', xaxis_title='Epoch', yaxis_title='Loss')
fig.show()

# Load the best model
model = Net(num_features, hidden_dim, num_classes)
model.load_state_dict(torch.load('best_model.pth'))

def train_gnn_model(gnn_model, data, epochs, plot=False, batched = False, name='trained_model.pth'):
    """
    Trains a Graph Neural Network (GNN) model using the provided data.

    Args:
        gnn_model (torch.nn.Module): The GNN model to be trained.
        data (torch_geometric.data.Data or torch_geometric.data.Batch): The input graph data.
        epochs (int): The number of training epochs.
        plot (bool, optional): If True, a loss curve plot will be displayed using Plotly. Default is False.
        batched (bool, optional): If True, assumes data is a batch of graphs; otherwise, data is a single graph.
                                 Default is False.
        name (str, optional): The name of the file to save the trained model. Default is 'trained_model.pth'.

    Returns:
        None: The function modifies the provided GNN model in-place.

    Example:
        >>> train_gnn_model(my_gnn_model, my_graph_data, epochs=50, plot=True, batched=False)
    """
    criterion = nn.MSELoss()  # Mean Squared Error Loss for regression
    optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.001)
    min_loss = float('inf')

    # Store loss values for plotting
    losses = []

    if not batched:
        for epoch in range(epochs):
            gnn_model.train()

            # Forward pass
            optimizer.zero_grad()
            z = gnn_model.encode(data.x, data.edge_index)
            neg_edge_index = negative_sampling(edge_index=data.edge_index, num_nodes=data.num_nodes,
                                            num_neg_samples=None, method='sparse')
            edge_label_index = torch.cat([data.edge_index, neg_edge_index], dim=-1, )
            edge_label = torch.cat([torch.ones(data.edge_index.size(1)), torch.zeros(neg_edge_index.size(1))],
                                dim=0)
            out = gnn_model.decode(z, edge_label_index).view(-1)
            loss = criterion(out, edge_label)
            loss.backward()
            optimizer.step()
            if loss.item() < min_loss:
                min_loss = loss.item()
                torch.save(gnn_model.state_dict(), name)

            # Append current loss to the list
            losses.append(loss.item())
            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')

        # Plotting the loss curve using Plotly
        if plot:
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=list(range(1, epochs + 1)), y=losses, mode='lines+markers'))
            fig.update_layout(title='Loss Over Epochs', xaxis_title='Epoch', yaxis_title='Loss')
            fig.show()

    else:
        for epoch in range(epochs):
            gnn_model.train()

            for i in len(data):
                # Forward pass
                optimizer.zero_grad()
                z = gnn_model.encode(data[i].x, data[i].edge_index)
                neg_edge_index = negative_sampling(edge_index=data[i].edge_index, num_nodes=data[i].num_nodes,
                                                num_neg_samples=None, method='sparse')
                edge_label_index = torch.cat([data[i].edge_index, neg_edge_index], dim=-1, )
                edge_label = torch.cat([torch.ones(data[i].edge_index.size(1)), torch.zeros(neg_edge_index.size(1))],
                                    dim=0)
                out = gnn_model.decode(z, edge_label_index).view(-1)
                loss = criterion(out, edge_label)
                loss.backward()
                optimizer.step()
                if loss.item() < min_loss:
                    min_loss = loss.item()
                    torch.save(gnn_model.state_dict(), name)

                # Append current loss to the list
                losses.append(loss.item())
                print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item()}')

        # Plotting the loss curve using Plotly
        if plot:
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=list(range(1, epochs + 1)), y=losses, mode='lines+markers'))
            fig.update_layout(title='Loss Over Epochs', xaxis_title='Epoch', yaxis_title='Loss')
            fig.show()

"""## Evaluation"""

def find_best_threshold(out, edge_label):
  """
  Finds the best threshold for a negative/positive edges classification task.

  This function iterates over a range of thresholds and calculates the error
  for each threshold. The threshold that minimizes the error is then selected
  as the best threshold.

  Parameters:
    - out (torch.Tensor): Model output tensor.
    - edge_label (torch.Tensor): Ground truth edge labels.

  Returns:
  - Tuple (float, float): A tuple containing the best threshold and the corresponding
    error rate.

  Example:
  ```python
  best_threshold, error_rate = find_best_threshold()
  print(f"Best Threshold: {best_threshold}, Error Rate: {error_rate}")
  ```
  """
  threshold = 1
  error = 1
  for i in np.arange(0, 1, 0.05):
    mask = (out > (torch.mean(out))*i).float()
    new_error = torch.sum(mask != edge_label).item() / len(edge_label)
    if new_error < error:
      threshold = i
      error = new_error
  return round(i, 2), error

threshold, error = find_best_threshold(out, edge_label)
print(threshold, error)

testing_data.x = testing_data.x.to(torch.float)
testing_data.edge_index = testing_data.edge_index.to(torch.int64)

print(testing_data.x.shape)
print(testing_data.edge_index)

from sklearn.metrics import roc_auc_score, f1_score
model.eval()
scores = []
z = model.encode(testing_data.x, testing_data.edge_index)
out = model.decode(z, testing_data.edge_index).view(-1)
pred = (out > torch.mean(out)*threshold).float() * 1
score = f1_score(np.ones(testing_data.edge_index.size(1)), pred.cpu().numpy())
scores.append(score)

print(score)

"""## Predictions"""

model

def make_a_prediction(gnn_model, max = 1000000, origin=None, destination=None, printing=False):
    """
    Generates predictions of potential edges in a graph using the previously trained GNN model.

    Parameters:
    - gnn_model (Net Class): The model with 'encode' and 'decode' functions to make predictions
    - max (int, optional): Maximum number of predicted edges to generate. Defaults to 1000000.
    - origin (int, optional): If provided, filters the predictions to include only edges originating from this node.
    - destination (int, optional): If provided, filters the predictions to include only edges leading to this node.
    - printing (bool, optional): If True, prints the number of possible edges found in each iteration.

    Returns:
    - numpy.ndarray: Predicted edges in the format of a NumPy array with two columns representing edge indices.

    Note:
    - The function stops predicting edges if the maximum limit (max) is reached.

    Example Usage:
    ```python
    predicted_edges = make_a_prediction(max=1000, origin=3, destination=7, printing=True)
    ```
    """
    pred_edges = []

    for i in range(len(subDatas)):
        z = gnn_model.encode(subDatas[i].x, subDatas[i].edge_index)
        neg_edge_index = negative_sampling(edge_index = subDatas[i].edge_index, num_nodes = None, \
                                           num_neg_samples = None, method = 'sparse')
        out = gnn_model.decode(z, neg_edge_index)
        pred = ((out > torch.mean(out)*threshold).float()).cpu().numpy()
        found = np.argwhere(pred == 1)
        if printing:
          print("Found: {z} possible edges in iteration {j}".format(z=found.size, j=i))
        if found.size > 0:
            edge_tuples = neg_edge_index.t().cpu().numpy()
            select_index = found.reshape(1, found.size)[0]
            edges = edge_tuples[select_index]
            pred_edges += edges.tolist()
            if len(pred_edges) >= max:
                break

    if origin is not None and destination is not None:
      pred_edges = np.array(pred_edges)
      index = np.where((pred_edges[:, 0] == origin) & (pred_edges[:, 1] == destination))
      if index[0].size == 0:
          print("Model predicted no connection between {i}, {j}".format(i=origin, j=destination))
      else:
          print("Found connection between {i}, {j} at {k}".format(i=origin, j=destination, k=index[0]))
      return pred_edges
    else:
      return pred_edges

    return None

pred = make_a_prediction(model, origin=8, destination=2)

print(len(pred))

pred[22]

""" # XGBoost

 Now that the GNN model can predict wheter it's most likely that there will be a connection between nodes in the graph, we can now apply a regression non-linear model to predict the weights of such connections.
"""

final_weights_matrix

print(x.shape, final_weights_matrix.shape)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install xgboost

del z
del x_tensor
del selected_nodes
del row_mask
del pred
del out
del data
del edge_index_tensor
del edge_label_index
del edge_label
del edge_index
del neg_edge_index

gc.collect()

del batch_edge_index
del batch_data
del loss
gc.collect()

"""## Training"""

import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, final_weights_matrix, test_size=0.2, random_state=42)

# Convert the data into DMatrix format, which is the internal data structure used by XGBoost
dtrain = xgb.DMatrix(x_train, label=y_train)
dtest = xgb.DMatrix(x_test, label=y_test)

# Define XGBoost parameters
params = {
    'objective': 'reg:squarederror',
    'max_depth': 2,  # Maximum depth of a tree.
    'learning_rate': 0.1,
    'n_estimators': 50 # Number of trees
}

model2 = xgb.train(params, dtrain)
model2.save_model('xgboost_model.model')
model2 = xgb.Booster()
model2.load_model('xgboost_model.model')

loaded_predictions = model2.predict(dtest)
mse = mean_squared_error(y_test, loaded_predictions)
print(f'Mean Squared Error on Test Set: {mse}')

x_test

"""## Prediction"""

def make_weight_prediction(xgboost_model ,job, final_weights_matrix, printing = False):
  """
  Make weight predictions using an XGBoost model.

  Parameters:
  - xgboost_model (xgb.Booster): The trained XGBoost model used for predictions.
  - job (xgb.DMatrix or array-like): The input data for which weight predictions are to be made.
      If not already an xgb.DMatrix, it will be converted to one.
  - printing (bool, optional): If True, print additional information about the predictions.
      Default is False.

  Returns:
  - predictions (numpy.ndarray): The predicted output, representing the correlation
    of the new data for all existing nodes.

  Example:
  >>> model = xgb.Booster()  # Replace with your trained XGBoost model
  >>> input_data = np.array([[...]])  # Replace with your input data
  >>> predictions = make_weight_prediction(model, input_data, printing=True)
  """
  if not isinstance(job, xgb.DMatrix):
    job = xgb.DMatrix(job)
  predictions = xgboost_model.predict(job)
  if printing:
    print("Predicted output (Correlation of the new data for all existing nodes):", predictions, predictions.shape)
    print("Difference mean: ", np.mean(predictions - final_weights_matrix[51]))
  return predictions

# Input array must have a shape of (1, 255)
predictions = make_weight_prediction(model2, np.array(x[51])[np.newaxis, :], True)

"""# Final Combined Model

Now that we have bot GNN that predicts all the possible edges between nodes inside the graph, and we also the XGBoost model that predicts edges weights, we can now create the ending point of our model that will be used for making either whole graph edges and edges weights predictions, or, to make an specific prediction of the edge and weight between node A and B.
"""

edge_index = testing_data.edge_index
start_idx = 15300
end_idx = 15519

# Create a boolean mask for rows where values are within the range [start_idx, end_idx]
row_mask = (edge_index[0, :] >= start_idx) & (edge_index[0, :] < end_idx) \
        & (edge_index[1, :] >= start_idx) & (edge_index[1, :] < end_idx)

# Apply the boolean mask to filter rows
batch_edge_index = edge_index[:, row_mask]
batch_y = testing_data.y[row_mask]

# Create a new Data object for sampling
sample_data = Data(x=testing_data.x, edge_index=batch_edge_index, y=batch_y)
testing_data

sample_data

"""Saving this data for Unit Testing"""

import pickle
with open('unit_testing_graph_data', 'wb') as f:
    pickle.dump(sample_data, f)

with open('unit_testing_graph_data', 'rb') as f:
    loaded_data = pickle.load(f)

loaded_data

def predict_edges_and_weights(threshold, xgboost_model, gnn_model, data, printing=False, origin=None, destination=None):
  """
  Predicts potential edges and their weights using a combination of graph neural network (GNN) and XGBoost models.

  Parameters:
  - threshold (float): The threshold value to determine the presence of an edge based on the GNN predictions.
  - xgboost_model: The pre-trained XGBoost model for edge weight prediction.
  - gnn_model: The pre-trained GNN model for edge presence prediction.
  - data: The input graph data containing features (data.x), edge indices (data.edge_index), and other information.
  - printing (bool, optional): If True, print additional information during execution. Default is False.
  - origin (int, optional): If provided, checks for predicted edges originating from this node.
  - destination (int, optional): If provided, checks for predicted edges leading to this node.

  Returns:
  - pred_edges (list): List of predicted edges in the format [(node1, node2), ...].
  - pred_weights (list): List of predicted weights corresponding to each edge in pred_edges.

  If origin and destination are provided:
  - If a connection is found between the specified nodes, returns (pred_edges, pred_weights).
  - If no connection is found, prints a message and returns None.

  If origin and destination are not provided:
  - Returns (pred_edges, pred_weights).

  If no predicted edges are found, returns None.
  """
  pred_edges = []
  pred_weights = []
  z = gnn_model.encode(data.x, data.edge_index)
  neg_edge_index = negative_sampling(edge_index=data.edge_index, num_nodes=data.num_nodes,
                                      num_neg_samples=None, method='sparse')
  edge_label_index = torch.cat([data.edge_index, neg_edge_index], dim=-1, )
  edge_label = torch.cat([torch.ones(data.edge_index.size(1)), torch.zeros(neg_edge_index.size(1))],
                          dim=0)
  out = gnn_model.decode(z, edge_label_index).view(-1)
  pred = ((out > torch.mean(out)*threshold).float()).cpu().numpy()
  if printing:
    edge_label = torch.cat([torch.ones(data.edge_index.size(1)), torch.zeros(neg_edge_index.size(1))],
                           dim=0)
    print("Error = {z}".format(z=\
    torch.sum((out > torch.mean(out)*threshold).float() != edge_label).item() / len(edge_label)))
  found = np.argwhere(pred == 1)
  if printing:
    print("Found: {z} possible edges".format(z=found.size))
  if found.size > 0:
      edge_tuples = edge_label_index.t().cpu().numpy()
      select_index = found.reshape(1, found.size)[0]
      pred_edges += edge_tuples[select_index].tolist()

  for count, _ in enumerate(tqdm(pred_edges, desc="Processing Edges", unit="edge")):
    weights_matrix = make_weight_prediction(xgboost_model, np.array(data.x[pred_edges[count][0]])[np.newaxis, :],\
                                            printing)
    edge_weight = list(weights_matrix)[0][pred_edges[count][1]]
    pred_weights.append(edge_weight)

  if origin is not None and destination is not None:
    pred_edges = np.array(pred_edges)
    index = np.where((pred_edges[:, :] == origin) & (pred_edges[:, :] == destination))
    if index[0].size == 0:
        print("Model predicted no connection between {i}, {j}".format(i=origin, j=destination))
    else:
        print("Found connection between {i}, {j} at {k}".format(i=origin, j=destination, k=index[0]))
    return pred_edges, pred_weights
  else:
    return pred_edges, pred_weights

  return None

predictions2 = predict_edges_and_weights(threshold, model2, model, sample_data, printing=True, origin=15450, destination=15511)

print(type(predictions2), predictions2[0].shape, len(predictions2[1]))

def predictions_to_df(predictions):
  results = np.array(predictions[0])
  results = np.hstack((results,  np.array(predictions[1]).reshape(len(predictions[1]),1)))
  results = pd.DataFrame(results, columns=['Origin', 'Destination', 'Weight'])
  results['Weight %'] = round(results['Weight'] * 100,2)
  results.iloc[:,0] = results.iloc[:,0].astype(int)
  results.iloc[:,1] = results.iloc[:,1].astype(int)
  return results

results = predictions_to_df(predictions2)

results

"""## Merging Results

Now I merge the new edges and edge_weights with the original 'sample_data'
"""

new_edges = torch.tensor([results['Origin'].values, results['Destination'].values])
new_edges.shape

sample_data.edge_index = torch.cat((sample_data.edge_index , new_edges), dim=1)
sample_data.edge_index.shape

sample_data.y = torch.cat((torch.from_numpy(sample_data.y), torch.tensor(results['Weight'].values)), dim=0)
sample_data.y.shape

sample_data

"""# Unit Testing

This Test Suite runs with python3 from the 'src' folder located in the [Official Github Repository of this Project](https://github.com/papitaAlgodonCplusplus/Linkedin-Graph-Neural-Networks-Project), instructions of how to run this Unit Test Suite can be found inside the **'README'** file, **'Running test suite'** section.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile test_suite
# 
# import unittest
# import gc
# import numpy as np
# import pandas as pd
# import pickle
# import xgboost as xgb
# import torch
# import networkx as nx
# import torch.nn.functional as F
# import torch_geometric.transforms as T
# import torch.nn as nn
# from torch_geometric.utils import negative_sampling
# from tqdm import tqdm
# from torch_geometric.nn import GCNConv
# from torch_geometric.data import Data
# 
# from functions.net_class import Net
# from functions.model_trainer import train_gnn_model, find_best_threshold, make_weight_prediction, train_xgboost_model
# 
# class ModelTesting(unittest.TestCase):
#     def setUp(self):
#         print("\nSetting up resources for the test")
#         self.num_features = 255
#         self.hidden_dim = 64
#         self.num_classes = 1
# 
#     def test_encoding(self):
#         gnn_model = Net(self.num_features, self.hidden_dim, self.num_classes)
#         gnn_model.load_state_dict(torch.load('models/best_model.pth'))
#         with open('test_data/unit_testing_graph_data', 'rb') as f:
#             loaded_data = pickle.load(f)
#             result = gnn_model.encode(loaded_data.x, loaded_data.edge_index)
#         # Ensure that encoding's output shape is [NUM_NODES, NUM_CLASSES = 1]
#         self.assertEqual(result.shape, torch.Size([len(loaded_data.x), self.num_classes]))
# 
#     def test_decoding(self):
#         gnn_model = Net(self.num_features, self.hidden_dim, self.num_classes)
#         gnn_model.load_state_dict(torch.load('models/best_model.pth'))
#         with open('test_data/unit_testing_graph_data', 'rb') as f:
#             loaded_data = pickle.load(f)
#             z = gnn_model.encode(loaded_data.x, loaded_data.edge_index)
#             neg_edge_index = negative_sampling(edge_index=loaded_data.edge_index, num_nodes=loaded_data.num_nodes,
#                                            num_neg_samples=None, method='sparse')
#             edge_label_index = torch.cat([loaded_data.edge_index, neg_edge_index], dim=-1, )
#             edge_label = torch.cat([torch.ones(loaded_data.edge_index.size(1)), torch.zeros(neg_edge_index.size(1))],
#                                 dim=0)
#             # Ensure that edge_label has shape of RNS's shape + NUM_EDGES
#             self.assertEqual(edge_label.shape[0], loaded_data.edge_index.shape[1] * 2)
# 
#             result = gnn_model.decode(z, edge_label_index).view(-1)
# 
#             # Ensure that predictions and labels are compatible
#             self.assertEqual(result.shape[0], edge_label.shape[0])
# 
#     def test_RNS(self):
#         with open('test_data/unit_testing_graph_data', 'rb') as f:
#             loaded_data = pickle.load(f)
#             neg_edge_index = negative_sampling(edge_index=loaded_data.edge_index, num_nodes=loaded_data.num_nodes,
#                                             num_neg_samples=None, method='sparse')
#             # Ensure that random negative sampling returned torch of shape [(Origin, Destination), NUM_EDGES]
#             self.assertEqual(neg_edge_index.shape, torch.Size([2, loaded_data.edge_index.shape[1]]))
#             # Ensure that no random negative sampled edge is a positive one
#             self.assertEqual(((neg_edge_index[0] == loaded_data.edge_index[0]) & (neg_edge_index[1] == loaded_data.edge_index[1]))\
#                              .nonzero().sum().item(), 0)
# 
#     def test_random_input_handling(self):
#         gnn_model = Net(5, self.hidden_dim, self.num_classes)
#         # 10 nodes, 30% probability of edge between each pair of nodes
#         graph = nx.erdos_renyi_graph(10, p=0.3)
#         edge_index = torch.tensor(list(graph.edges)).t().contiguous()
#         # 10 nodes, 5 features
#         x = torch.randn(10, 5)
#         random_data = Data(x=x, edge_index=edge_index)
#         train_gnn_model(gnn_model, random_data, 5)
# 
#         z = gnn_model.encode(random_data.x, random_data.edge_index)
#         self.assertIsNotNone(z)
#         self.assertFalse(torch.isnan(z).any())
# 
#         neg_edge_index = negative_sampling(edge_index = random_data.edge_index, num_nodes = None, \
#                                            num_neg_samples = None, method = 'sparse')
# 
#         out = gnn_model.decode(z, neg_edge_index)
#         self.assertIsNotNone(out)
#         self.assertFalse(torch.isnan(out).any())
# 
#     def test_gnn_model_accuracy(self):
#         gnn_model = Net(self.num_features, self.hidden_dim, self.num_classes)
#         gnn_model.load_state_dict(torch.load('models/best_model.pth'))
#         with open('test_data/unit_testing_graph_data', 'rb') as f:
#             loaded_data = pickle.load(f)
#             z = gnn_model.encode(loaded_data.x, loaded_data.edge_index)
#             neg_edge_index = negative_sampling(edge_index=loaded_data.edge_index, num_nodes=loaded_data.num_nodes,
#                                            num_neg_samples=None, method='sparse')
#             edge_label_index = torch.cat([loaded_data.edge_index, neg_edge_index], dim=-1, )
#             edge_label = torch.cat([torch.ones(loaded_data.edge_index.size(1)), torch.zeros(neg_edge_index.size(1))],
#                                 dim=0)
#             result = gnn_model.decode(z, edge_label_index).view(-1)
#             threshold, error = find_best_threshold(result, edge_label)
# 
#             # Ensure at least 70% accuracy on binary edge predictions
#             self.assertLessEqual(error, 0.3)
# 
#     def test_xgboost_accuracy(self):
#         # 50 nodes, 30 features
#         x = np.random.rand(50, 30)
# 
#         # Set the first column to be the row indices
#         x[:, 0] = np.arange(50)
# 
#         # Convert values in the second to the last columns to 0 or 1
#         x[:, 1:] = np.random.choice([0, 1], size=(50, 29))
# 
#         # Generate random weights matrix
#         original_weights_matrix = np.random.rand(50, 30)
# 
#         train_xgboost_model(x, original_weights_matrix)
#         xgboost_model = xgb.Booster()
#         xgboost_model.load_model('models/xgboost_model_unit_testing.json')
#         resulting_weights, mean_error = make_weight_prediction(0, original_weights_matrix, xgboost_model, \
#                                                                 np.array([original_weights_matrix[0]]), False, True)
# 
#         # Ensure at least 95% accuracy on edges weights predictions
#         self.assertGreaterEqual(1-mean_error, 0.95)
# 
#     def tearDown(self):
#         print("\nCleaning up resources after the test")
#         del self.num_features
#         del self.num_classes
#         del self.hidden_dim
#         gc.collect()
# 
# def layer_test():
#     suite = unittest.TestSuite()
#     suite.addTest(ModelTesting('test_encoding'))
#     suite.addTest(ModelTesting('test_RNS'))
#     suite.addTest(ModelTesting('test_decoding'))
#     return suite
# 
# def compatibility_test():
#     suite = unittest.TestSuite()
#     suite.addTest('test_random_input_handling')
#     return suite
# 
# def accuracy_test():
#     suite = unittest.TestSuite()
#     suite.addTest('test_gnn_model_accuracy')
#     suite.addTest('test_xgboost_accuracy')
#     return suite
# 
# if __name__ == '__main__':
#     test_loader = unittest.TestLoader()
#     test_suite = test_loader.discover('.')
# 
#     runner = unittest.TextTestRunner()
#     runner.run(test_suite)

"""# GNNLens2"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install Flask==2.0.3
# !pip install gnnlens
# !pip install dgl

print(dgl.__version__)

"""## Data Generation"""

edge_index = testing_data.edge_index
start_idx = 14000
end_idx = 15000

# Create a boolean mask for rows where values are within the range [start_idx, end_idx]
row_mask = (edge_index[0, :] >= start_idx) & (edge_index[0, :] < end_idx) \
        & (edge_index[1, :] >= start_idx) & (edge_index[1, :] < end_idx)

# Apply the boolean mask to filter rows
batch_edge_index = edge_index[:, row_mask]
batch_y = testing_data.y[row_mask]
batch_x = testing_data.x[start_idx:end_idx, :]

# Create a new Data object for sampling
sample_gnnlens2_data = Data(x=batch_x, edge_index=batch_edge_index, y=batch_y)

# Nodes of this Data have index 0 to 1000
sample_gnnlens2_data.edge_index = sample_gnnlens2_data.edge_index - start_idx

sample_gnnlens2_data

import pickle
with open('sample_gnnlens2_data', 'wb') as f:
    pickle.dump(sample_gnnlens2_data, f)

predictions = predict_edges_and_weights(threshold, model2, model, sample_gnnlens2_data, printing=True)

results = predictions_to_df(predictions)

results



new_edges = torch.tensor([results['Origin'].values, results['Destination'].values])
new_edges.shape

sample_gnnlens2_data.edge_index = torch.cat((sample_gnnlens2_data.edge_index , new_edges), dim=1)
sample_gnnlens2_data.edge_index.shape

sample_gnnlens2_data.y = torch.cat((torch.from_numpy(sample_gnnlens2_data.y), torch.tensor(results['Weight'].values)), dim=0)
sample_gnnlens2_data.y.shape

import pickle
with open('predicted_new_data', 'wb') as f:
    pickle.dump(sample_gnnlens2_data, f)

"""## Script"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile gnnlens2_main_app.py
# import gnnlens
# import torch
# import pickle
# import shutil
# import os
# from torch_geometric.data import Data
# from dgl import DGLGraph
# from gnnlens import Writer
# import numpy as numpy
# 
# # Data(x=[15520, 255], edge_index=[2, 9692], y=[9692])
# with open('test_data/sample_gnnlens2_data', 'rb') as f:
#     original_data = pickle.load(f)
# original_data.edge_attr = torch.from_numpy(original_data.y)
# original_data.y = None
# 
# # Data(x=[15520, 255], edge_index=[2, 13051], y=[13051])
# with open('test_data/predicted_new_data', 'rb') as f:
#     predicted_new_data = pickle.load(f)
# predicted_new_data.edge_attr = predicted_new_data.y
# predicted_new_data.y = None
# 
# # Function to convert torch_geometric Data to DGLGraph
# def torch_geometric_to_dgl(data):
#     dgl_graph = DGLGraph()
#     dgl_graph.add_nodes(data.num_nodes)
#     dgl_graph.add_edges(data.edge_index[0], data.edge_index[1])
# 
#     # Copy node features
#     dgl_graph.ndata['x'] = data.x
# 
#     # Copy edge features if available
#     if 'edge_attr' in data:
#         dgl_graph.edata['edge_attr'] = data.edge_attr
# 
#     return dgl_graph
# 
# original_graph = torch_geometric_to_dgl(original_data)
# predicted_graph = torch_geometric_to_dgl(predicted_new_data)
# 
# if os.path.exists("sample_gnnlens2_app"):
#     try:
#         shutil.rmtree("sample_gnnlens2_app")
#     except Exception as e:
#         print(f"Error: Unable to delete the folder 'sample_gnnlens2_app'.")
#         print(e)
# 
# # Specify the path to create a new directory for dumping data files.
# writer = Writer('sample_gnnlens2_app')
# writer.add_graph(name='Sample DGLGraph Data', graph=original_graph, eweights={"edge_weights": \
#                                                                          original_data.edge_attr.view(len(original_data.edge_attr))})
# writer.add_graph(name='Predicted DGLGraph Data', graph=predicted_graph, eweights={"edge_weights": \
#                                                                          predicted_new_data.edge_attr.view(len(predicted_new_data.edge_attr))})
# # Finish dumping
# writer.close()

"""By entering localhost:7777 in your web browser address bar, you can see the GNNLens2 interface like below. 7777 is the default port GNNLens2 uses. You can specify an alternative one by adding --port xxxx after the command line and change the address in the web browser accordingly."""

!python gnnlens2_main_app.py
!gnnlens --logdir sample_gnnlens2_app